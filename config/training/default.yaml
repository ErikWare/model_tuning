# Training configuration
training:
  num_epochs: 100
  batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 3e-5
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  save_steps: 1000
  eval_steps: 500
  save_total_limit: 2  # Maximum number of checkpoints to keep
  logging_steps: 100
  logging_dir: ${paths.log_dir}/runs
  load_best_model_at_end: true
  metric_for_best_model: "loss"
  greater_is_better: false

# Optimizer configuration
optimizer:
  type: AdamW
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8

# Learning rate scheduler
scheduler:
  type: linear
  num_warmup_steps: ${training.warmup_steps}
  num_training_steps: null  # Will be set during training

# Data configuration
data:
  train_batch_size: ${training.batch_size}
  eval_batch_size: ${training.batch_size}
  shuffle: true
  num_workers: 4

# Validation configuration
validation:
  validation_split: 0.1
  eval_batch_size: ${training.batch_size}
  eval_accumulation_steps: 1
  metrics:
    - loss
    - perplexity

# Early stopping
early_stopping:
  enabled: true
  patience: 3
  threshold: 0.01
