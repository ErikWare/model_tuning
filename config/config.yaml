defaults:
  - model: gpt2
  - training: default
  - _self_

# Project configuration
project:
  name: model_tuning
  seed: 42
  device: auto  # Will automatically detect GPU/CPU
  save_strategy: "steps"
  save_steps: 1000
  evaluation_strategy: "steps"
  eval_steps: 500

# Paths configuration
paths:
  data_dir: ${hydra:runtime.cwd}/data
  raw_data: ${paths.data_dir}/raw
  processed_data: ${paths.data_dir}/processed
  model_dir: ${hydra:runtime.cwd}/models
  output_dir: ${hydra:runtime.cwd}/outputs
  log_dir: ${paths.output_dir}/logs

# Weights & Biases configuration
wandb:
  project: ${project.name}
  entity: ${oc.env:WANDB_ENTITY}
  mode: online  # Set to "disabled" to turn off
  tags: []

# Logging configuration
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  to_file: true
  filename: ${paths.log_dir}/experiment.log

# Runtime configuration
runtime:
  mixed_precision: "fp16"
  gradient_checkpointing: false
  distributed_training: false

# Model configuration
model:
  model_path: "gpt2"  # or path to your pretrained model
  tokenizer_path: "gpt2"  # or path to your tokenizer
  save_path: "outputs/math_model"

# Data configuration
data:
  path: "data/math_problems.jsonl"  # This path is relative to project root

# Training configuration
training:
  output_dir: "outputs"
  epochs: 5
  batch_size: 4
  warmup_steps: 100
  learning_rate: 2e-5
  logging_dir: "logs"
  logging_steps: 10
  save_steps: 500
  save_total_limit: 2

hydra:
  run:
    dir: ${paths.output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${paths.output_dir}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: true
